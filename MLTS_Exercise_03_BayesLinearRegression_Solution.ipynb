{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MLTS Exercise 02 - Bayesian Linear Regression</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "* Implement the log marginal likelihood for training a Bayesian linear regression model (`linregFitBayes`)\n",
    "\n",
    "* Implement the prediction for Bayesian linear regression (`linregPredictBayes`)\n",
    "\n",
    "* Learn the parameters of a linear regression for some noisy measurements of a function by maximizing log marginal likelihood w.r.t. to the parameters\n",
    "\n",
    "* Use the trained model to make predictions on previously unseen test data\n",
    "\n",
    "\n",
    "### Basic variables\n",
    "\n",
    "* parameters: $\\alpha, \\beta$\n",
    "\n",
    "* weights: $w$\n",
    "\n",
    "* Datapoints: $N$\n",
    "\n",
    "### Helpfull intermediate variables\n",
    "\n",
    "* $X^* = X^TX$\n",
    "\n",
    "* $X_y = X^Ty$\n",
    "\n",
    "### Needed Variables\n",
    "\n",
    "* $A = αI + βX^*$\n",
    "\n",
    "* $U = cholesky(A)^T$\n",
    "\n",
    "* $U_i = U^{-1}$\n",
    "\n",
    "* $A^{-1 }= U_iU_i^T$\n",
    "\n",
    "* $m_N = β\\ A^{-1}\\ X_y$\n",
    "\n",
    "* $E(m_N) = \\frac{β}{2}\\parallel y - X\\ m_N \\parallel^2 + \\frac{α}{2}\\ m_N^T\\ m_N$\n",
    "\n",
    "#### Log marginal likelihood\n",
    "\n",
    "* $ln \\textrm{ } p(y|X) = \\frac{M}{2}\\ ln\\ α + \\frac{N}{2}\\ ln\\ β − E(m_N) − \\frac{1}{2}\\ ln\\ |A| − \\frac{N}{2}\\ ln(2π)$\n",
    "\n",
    "* $α = \\cfrac{\\gamma}{m_N^Tm_N}$\n",
    "\n",
    "* $\\gamma = M - α tr(A^{-1})$\n",
    "\n",
    "* $β = \\cfrac{N - \\gamma}{\\sum^N_{n=1}(y_n - m^T_N x_n)^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def polyBasis(x: np.array, deg: int) -> np.array:\n",
    "    \"\"\"Takes a vector and returns a polynomial basis matrix up to degree deg\n",
    "\n",
    "    Args:\n",
    "        x (np.array): Vector\n",
    "        deg (int): Degree\n",
    "\n",
    "    Returns:\n",
    "        np.array: Polynomial basis matrix\n",
    "    \"\"\"\n",
    "    return (np.column_stack([x ** deg for deg in range(deg + 1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Task: Implement the log marginal likelihood for training a Bayesian linear regression model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linregFitBayes(X: np.array, y: np.array, maxIter: int) -> tuple[dict, float, np.array]:\n",
    "    \"\"\"Learn a Bayes regression model by optimizing log marginal likelihood.\n",
    "\n",
    "    Args:\n",
    "        X (np.array): Datapoints.\n",
    "        y (np.array): Observed y.\n",
    "        maxIter (int): Maximal numbers of iterations.\n",
    "\n",
    "    Returns:\n",
    "        tuple[dict, float, np.array]: Model, last value & history of log marginal likelihood\n",
    "    \"\"\"\n",
    "    # we can start with some initial values for alpha and beta and then change them iteratively based on data\n",
    "    alpha = 0.01\n",
    "    beta = 1\n",
    "\n",
    "    log_ml_old = - float('inf') # initialize log marginal likelihood\n",
    "    Lhist = np.empty((maxIter, 1))\n",
    "\n",
    "    # implement the log marginal likelihood logp(Y|X) and for a fixed number of iteration optimize it\n",
    "    # at each loop iteration compare new (L) and old (L_old) log marginal likelihood\n",
    "    # if abs(L - L_old) < 1e-2 stop the optimization\n",
    "\n",
    "    # Initialize some variables\n",
    "    N, M = X.shape\n",
    "\n",
    "    # (helper variables)\n",
    "    XX = np.dot(np.transpose(X), X)\n",
    "    Xy = np.dot(np.transpose(X), y)\n",
    "\n",
    "    for i in range(maxIter):\n",
    "\n",
    "        A = alpha * np.identity(M) + beta * XX\n",
    "        U = np.transpose(np.linalg.cholesky(A))\n",
    "        U_inv = np.linalg.inv(U)\n",
    "        A_inv = np.dot(U_inv, np.transpose(U_inv))\n",
    "\n",
    "        m_N = beta * np.dot(A_inv, Xy)\n",
    "\n",
    "        t1 = sum((y - np.dot(X, m_N)) * (y - np.dot(X, m_N)))\n",
    "        t2 = np.dot(np.transpose(m_N), m_N)\n",
    "\n",
    "        E_mn = beta * t1 - alpha * t2\n",
    "\n",
    "        # https://xcorr.net/2008/06/11/log-determinant-of-positive-definite-matrices-in-matlab/\n",
    "        log_det = 2 * sum(np.log(np.diag(U)))\n",
    "        log_ml = float(M) * np.log(alpha) \\\n",
    "                 + float(N) * np.log(beta) \\\n",
    "                 - E_mn \\\n",
    "                 - log_det \\\n",
    "                 - float(N) * np.log(2 * np.pi)\n",
    "        log_ml = log_ml / 2.\n",
    "\n",
    "        gamma = M - alpha * np.trace(A_inv)\n",
    "        alpha = gamma / t2\n",
    "        beta = (N - gamma) / t1\n",
    "\n",
    "        Lhist[i] = log_ml\n",
    "        if abs(log_ml - log_ml_old) < 1e-2:\n",
    "            break\n",
    "        else:\n",
    "            log_ml_old = log_ml\n",
    "\n",
    "    model = {'m_N': m_N, 'A_inv': A_inv, 'beta': beta, 'alpha': alpha, 'gamma': gamma}\n",
    "\n",
    "    return model, log_ml_old, Lhist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Task: Implement the prediction for Bayesian linear regression </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linregPredictBayes(model: dict, X: np.array) -> tuple:\n",
    "    \"\"\"This accepts a model of the form produced by linregFitBayes and \n",
    "        an array of X to form posterior predictions p(y∗|x∗,y,X)\n",
    "\n",
    "    Args:\n",
    "        model (dict): Trained Model.\n",
    "        X (np.array): Datapoints.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Predictions of mean & variance for datapoints.\n",
    "    \"\"\"\n",
    "    # X.shape = 140x3, m_N.shape = 3\n",
    "    yhat = np.dot(X, model[\"m_N\"])\n",
    "    sigma2hat = (1. / model[\"beta\"]) + np.diag(np.dot(np.dot(X, model[\"A_inv\"]), np.transpose(X)))\n",
    "\n",
    "    return yhat, sigma2hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Bayesian model selection demo for polynomial regression</h3>\n",
    "\n",
    "* **Task:** Learn model parameters based on training data using marginal likelihood\n",
    "* **Task:** Make predictions on previously unseen test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@widgets.interact(\n",
    "        n_data_points=widgets.IntSlider(min=10, max=50, step=10, value=30, description='Number of data points: '), \n",
    "        polynomial_degree=widgets.IntSlider(min=1, max=5, step=1, value=2, description='Degree of polynomial: '),\n",
    "        random_seed=widgets.IntSlider(min=0, max=4, step=1, value=0, description='Random seed: ')\n",
    ")\n",
    "def plot(n_data_points=30, polynomial_degree=2, random_seed=0):\n",
    "    # generate data and plots here\n",
    "    # Change the number of the random seed to get different data points\n",
    "    np.random.seed(random_seed)  # to generate the same random data in diffrenet runs\n",
    "\n",
    "    # --- Setup Dataset ---\n",
    "    # training x1d and test data plotvals1d\n",
    "    x1d = np.random.uniform(0, 10, n_data_points)  # input points\n",
    "    e = np.random.normal(0, 1, n_data_points)  # noise\n",
    "    ytrain = (x1d - 4.0) ** polynomial_degree + 5.0 * e  # observed y\n",
    "    plotvals1d = np.arange(-2.0, 12, .1)  # grid for plotting/testing\n",
    "    trueOutput = (plotvals1d - 4.0) ** polynomial_degree  # true function\n",
    "\n",
    "    # --- Learn the model parameters based on the training data using marginal likelihood ---\n",
    "    X = polyBasis(x1d, polynomial_degree)  # Polynomial basis\n",
    "    [mod, logev, hist] = linregFitBayes(X, ytrain, maxIter=20)  # Fit the model\n",
    "\n",
    "    # --- Make predictions on previously unseen data (test data) ---\n",
    "    Xtest = polyBasis(plotvals1d, polynomial_degree)  # Grid to test our prediction on\n",
    "    [mu, sig2] = linregPredictBayes(mod, Xtest)\n",
    "    sig2 = np.sqrt(sig2)\n",
    "\n",
    "    # --- Plot results ---\n",
    "    plt.close()\n",
    "    plt.plot()\n",
    "    plt.scatter(x1d, ytrain, s=140, facecolors='none', edgecolors='k', label='data')\n",
    "    lower = mu - sig2\n",
    "    upper = mu + sig2\n",
    "    plt.plot(plotvals1d, trueOutput, 'g', linewidth=2, label='true function')\n",
    "    plt.plot(plotvals1d, mu, 'r--', linewidth=2, label='predictive mean')\n",
    "    plt.plot(plotvals1d, lower, 'b-', linewidth=0.5)\n",
    "    plt.plot(plotvals1d, upper, 'b-', linewidth=0.5, label='predictive standard deviation')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "6ce972b8196c3bc344073d559e470958edd80946e92866da6533a1277d0d07bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
